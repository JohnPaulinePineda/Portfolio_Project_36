---
title: 'Unsupervised Learning : Discovering Latent Variables in High-Dimensional Data using Exploratory Factor Analysis'
author: "John Pauline Pineda"
date: "August 13, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```

# **1. Table of Contents**
|
| This project explores different variations of the exploratory factor analysis method for discovering latent patterns in adequately correlated high-dimensional data using various helpful packages in <mark style="background-color: #CCECFF">**R**</mark>. Methods applied in the analysis to estimate and identify potential underlying structures from observed variables included **Principal Axes Factor Extraction** and **Maximum Likelihood Factor Extraction**. The approaches used to simplify the derived factor structures to achieve a more interpretable pattern of factor loadings included **Varimax Rotation** and **Promax Rotation**. Combinations of the factor extraction and rotation methods were separately applied on the original dataset across different numbers of factors, with the model fit evaluated using the standardized root mean square of the residual, Tucker-Lewis fit index, Bayesian information criterion and high residual rate. The extracted and rotated factors were visualized using the factor loading and dandelion plots.
|
| Exploratory factor analysis is a form of unsupervised learning method aimed at uncovering underlying patterns or relationships among a set of observed variables by exploring the structure of the data and identifying latent factors that explain the observed correlations; and at reducing the complexity of the data by grouping related variables together under these latent factors. The algorithms applied in this study (mostly contained in the <mark style="background-color: #CCECFF">**psych**</mark>, <mark style="background-color: #CCECFF">**nFactors**</mark> and <mark style="background-color: #CCECFF">**parameters**</mark> packages) attempt to extract a smaller number of factors that influence the correlations among variables and apply factor rotation methods to transform the initially extracted factors into a more interpretable form.
|
##  1.1 Sample Data
|
| The [<mark style="background-color: #EEEEEE;color: #FF0000">**JobHiring**</mark>](https://statisticsbyjim.com/basics/factor-analysis/) dataset obtained from the [<mark style="background-color: #CCECFF">**Statistics By Jim**</mark>](https://statisticsbyjim.com/) website by [Jim Frost](https://statisticsbyjim.com/jim_frost/) was used for this illustrated example.   
|
| Preliminary dataset assessment:
|
| **[A]** 50 rows (observations)
| 
| **[B]** 12 columns (variables)
|      **[B.1]** 12/12 descriptors = 12/12 ordered category 
|             **[B.1.1]** <span style="color: #FF0000">ACAD</span> (Academic Record)
|             **[B.1.2]** <span style="color: #FF0000">APPR</span> (Appearance)
|             **[B.1.3]** <span style="color: #FF0000">COMM</span> (Communication)
|             **[B.1.4]** <span style="color: #FF0000">CFIT</span> (Company Fit)
|             **[B.1.5]** <span style="color: #FF0000">EXPR</span> (Experience)
|             **[B.1.6]** <span style="color: #FF0000">JFIT</span> (Job Fit)
|             **[B.1.7]** <span style="color: #FF0000">LETT</span> (Cover Letter)
|             **[B.1.8]** <span style="color: #FF0000">LIKE</span> (Likeability)
|             **[B.1.9]** <span style="color: #FF0000">ORGN</span> (Organization)
|             **[B.1.10]** <span style="color: #FF0000">POTL</span> (Potential)
|             **[B.1.11]** <span style="color: #FF0000">RESM</span> (Resume)
|             **[B.1.12]** <span style="color: #FF0000">SCON</span> (Self-Confidence)
|     
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(performance)
library(parameters)
library(HH)
library(tidyr)
library(caret)
library(psych)
library(lattice)
library(dplyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(factoextra)
library(FactoMineR)
library(gplots)
library(qgraph)
library(ggplot2)
library(psych)
library(nFactors)
library(MBESS)
library(DandEFA)
library(EFAtools)

##################################
# Loading source and
# formulating the analysis set
##################################
JobHiring <- read.csv("JobHiring.csv",
                   na.strings=c("NA","NaN"," ",""),
                   stringsAsFactors = FALSE)
JobHiring <- as.data.frame(JobHiring)

##################################
# Performing a general exploration of the data set
##################################
dim(JobHiring)
str(JobHiring)
summary(JobHiring)

##################################
# Formulating a data type assessment summary
##################################
PDA <- JobHiring
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

##  1.2 Data Quality Assessment
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** No low variance observed for any variable with First.Second.Mode.Ratio>5.
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- JobHiring

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}
```

</details>

##  1.3 Data Preprocessing

###  1.3.1 Outlier Detection
|
| **[A]** Outliers noted for 3 out of the 12 descriptors. Descriptor values were visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile).
|      **[A.1]** <span style="color: #FF0000">APPR</span> = 2
|      **[A.2]** <span style="color: #FF0000">LIKE</span> = 3
|      **[A.3]** <span style="color: #FF0000">SCON</span> = 4
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- JobHiring

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

```

```{r section_1.3.1.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Outlier Detection
##################################

##################################
# Listing all Descriptors
##################################
DPA.Descriptors <- DPA

##################################
# Listing all numeric Descriptors
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors[,sapply(DPA.Descriptors, is.numeric)]

##################################
# Identifying outliers for the numeric Descriptors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Descriptors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Descriptors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}


```

</details>

###  1.3.2 Zero and Near-Zero Variance
|
| **[A]** No low variance observed for any descriptor using a preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package. The <span style="color: #0000FF">nearZeroVar</span> method using both the <span style="color: #0000FF">freqCut</span> and <span style="color: #0000FF">uniqueCut</span> criteria set at 95/5 and 10, respectively, were applied on the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance descriptors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}

```

</details>

###  1.3.3 Collinearity
|
| **[A]** No multicollinearity with Pearson correlation coefficients >80% was noted among pairs of descriptors using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Visualizing pairwise correlation between descriptor
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "circle",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the highly correlated variables
##################################
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.90))

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.90)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

}



```

</details>

###  1.3.4 Linear Dependency
|
| **[A]** No linear dependencies noted for any subset of decriptors using the preprocessing summary from the <mark style="background-color: #CCECFF">**caret**</mark> package applying the <span style="color: #0000FF">findLinearCombos</span> method which utilizes the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}

```

</details>

###  1.3.5 Distributional Shape
|
| **[A]** No shape transformation was necessary as the distributional skewness observed among individual descriptors was normal (all values within +3 and -3) with minimal outliers.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Distributional Shape
##################################

##################################
# Formulating the histogram
# for the numeric descriptors
##################################
for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Median <- format(round(median(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA.Descriptors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Descriptors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

```

</details>

##  1.4 Data Pre-Assessment

###  1.4.1 Correlation Matrix Assessment - Covariance Validity
|
| [Covariance Validity](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) evaluates whether the ratio of associated variables in the data set are sufficient enough to support the assumption that correlations exist. The criterion is computed by determining the proportion of correlation coefficients with values of at least 30% between all pairs of variables in the data set. A value closer to 1 suggests an adequate percentage of pairwise-correlated variables.
|
| **[A]** Covariance among descriptors in the correlation matrix was sufficient to justify the conduct of an exploratory factor analysis. 94% (62/66) of the pairwise associations using the Pearson correlation coefficient were above 30%.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.1, warning=FALSE, message=FALSE}
##################################
# Identifying the minimally correlated variables
##################################
(DPA_MinimallyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.30))

(DPA_AllPairs <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(DPA_MinimallyCorrelatedCountPercentage <- DPA_MinimallyCorrelatedCount/DPA_AllPairs)

qgraph(cor(DPA.Descriptors.Numeric),
       cut=0.30,
       details=TRUE,
       posCol="#2F75B5",
       negCol="#FF5050",
       labels=names(DPA.Descriptors.Numeric))

```

</details>

###  1.4.2 Correlation Matrix Assessment - Determinant Computation
|
| [Determinant Computation](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) reflects the extent of multicollinearity among the variables in a correlation matrix. An extremely small determinant value indicates that the variables are highly correlated and nearly linearly dependent which can lead to unstable results and difficulty in interpreting their individual contributions.
|
| **[A]** The determinant of the correlation matrix was computed as 0.00002 (greater than .00001) indicating the absence of the likelihood for a multicollinearity problem. The results allowed for matrix operations to produce stable results during exploratory factor analysis.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.2, warning=FALSE, message=FALSE}

##################################
# Computing the determinant of the correlation matrix
##################################
(DPA_CorrelationMatrixDeterminant <- det(cor(DPA.Descriptors.Numeric)))

```

</details>

###  1.4.3 Correlation Matrix Assessment - Bartlett’s Test of Sphericity
|
| [Bartlett’s Test of Sphericity](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) evaluates whether the correlations between variables in a data set are significant enough to support the assumption that underlying factors exist and can be extracted. The test calculates a Chi-Square statistic based on the differences between the observed correlation matrix and an identity matrix. The larger the Chi-Square value, there is more evidence against the null hypothesis stating that the correlation matrix is an identity matrix which indicates that the variables are uncorrelated and do not have any underlying structure.
|
| **[A]** The computed p-value from the Bartlett’s Test of Sphericity was statistically significant (<0.00001), rejecting the null hypothesis that the correlation matrix is an identity matrix (ones on the diagonal and zeros on the off-diagonal). Results indicated that there is enough evidence to support the existence of underlying factors, suggesting that the correlation matrix is appropriate for exploratory factor analysis. 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.3, warning=FALSE, message=FALSE}
##################################
# Calculating the Bartlett's Test of Sphericity
##################################
(DPA_BartlettTest <- cortest.bartlett(DPA.Descriptors.Numeric,
                                      n=nrow(DPA.Descriptors.Numeric)))

```

</details>

###  1.4.4 Correlation Matrix Assessment - Kaiser-Meyer-Olkin Factor Adequacy
|
| [Kaiser-Meyer-Olkin Factor Adequacy](https://link.springer.com/article/10.1007/BF02291817) evaluates whether the observed variables are suitable for exploratory factor analysis based on their common variance and the potential for extracting meaningful factors. The criterion is computed by examining the ratio of the sum of squared correlations between variables to the sum of squared partial correlations. A KMO value closer to 1 suggests that the variables have high shared variance and are suitable to proceed with the analysis.
|
| **[A]** The KMO measure of sampling adequacy was acceptable with a computed value of 0.798 for the complete model, indicating the suitability of the data for exploratory factor analysis. The estimated proportion of variance among all the observed variable was sufficiently adequate.
|
| **[B]**  The sampling adequacy for each variable in the model was acceptable with KMO values ranging from 0.676 to 0.883. Results indicated that each variable can be sufficiently predicted by other variables. 
|      **[B.1]** <span style="color: #FF0000">ACAD</span> = 0.85
|      **[B.2]** <span style="color: #FF0000">APPR</span> = 0.88
|      **[B.3]** <span style="color: #FF0000">COMM</span> = 0.79
|      **[B.4]** <span style="color: #FF0000">CFIT</span> = 0.81
|      **[B.5]** <span style="color: #FF0000">EXPR</span> = 0.78
|      **[B.6]** <span style="color: #FF0000">JFIT</span> = 0.83
|      **[B.7]** <span style="color: #FF0000">LETT</span> = 0.68
|      **[B.8]** <span style="color: #FF0000">LIKE</span> = 0.85
|      **[B.9]** <span style="color: #FF0000">ORGN</span> = 0.74
|      **[B.10]** <span style="color: #FF0000">POTL</span> = 0.88
|      **[B.11]** <span style="color: #FF0000">RESM</span> = 0.71
|      **[B.12]** <span style="color: #FF0000">SCON</span> = 0.78
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.4, warning=FALSE, message=FALSE}
##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

```

</details>

## 1.5 Data Exploration
|
| **[A]** With the descriptors treated as ordered categorical variables, distribution patterns were observed as follows:
|      **[A.1]** As individual descriptors, <span style="color: #FF0000">ACAD</span>, <span style="color: #FF0000">APPR</span>, <span style="color: #FF0000">LIKE</span> and <span style="color: #FF0000">SCON</span> obtained higher proportions of likert scale scores of 6 and above
|      **[A.2]** As individual descriptors, <span style="color: #FF0000">CFIT</span> and <span style="color: #FF0000">ORGN</span> obtained higher proportions of likert scale scores of 5 and below
|
| **[B]** With the descriptors treated as numeric variables, all pairwise associations were positive with the highest Pearson correlation coefficient values noted for the following:
|      **[B.1]** <span style="color: #FF0000">JFIT</span> and <span style="color: #FF0000">CFIT</span> = 0.88
|      **[B.2]** <span style="color: #FF0000">COMM</span> and <span style="color: #FF0000">ORGN</span> = 0.86
|      **[B.3]** <span style="color: #FF0000">RESM</span> and <span style="color: #FF0000">LETT</span> = 0.84

|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5, warning=FALSE, message=FALSE}
##################################
# Percentage contribution
# among descriptors when treated
# as ordered categorical variables
##################################
DPA.Descriptors.Ordered <- gather(DPA.Descriptors.Numeric,
                                   "ACAD",
                                   "APPR",
                                   "COMM",
                                   "CFIT",
                                   "EXPR",
                                   "JFIT",
                                   "LETT",
                                   "LIKE",  
                                   "ORGN",
                                   "POTL",
                                   "RESM",
                                   "SCON",
                                   key="Likert_Variable",
                                   value="Likert_Score")


DPA.Descriptors.Ordered$Likert_Variable <- as.factor(DPA.Descriptors.Ordered$Likert_Variable)
DPA.Descriptors.Ordered$Likert_Score <- factor(DPA.Descriptors.Ordered$Likert_Score,                                               levels=c("1","2","3","4","5","6","7","8","9","10"))

(DPA.Descriptors.OrderedTable <- table(DPA.Descriptors.Ordered$Likert_Variable,
                                       DPA.Descriptors.Ordered$Likert_Score))

DPA.Descriptors.OrderedTableProportion <- as.data.frame(rbind(prop.table(DPA.Descriptors.OrderedTable,1)))
DPA.Descriptors.OrderedTableProportion$LikertVariables <- rownames(DPA.Descriptors.OrderedTableProportion)
DPA.Descriptors.OrderedTableProportion

likert(LikertVariables~., 
       DPA.Descriptors.OrderedTableProportion, 
       ReferenceZero=5.5, 
       ylab = "Likert Variables", 
       xlab = "Percentage Distribution of Likert Scale Scores",
       main = list("Variables Characterizing Candidates During Job Hiring"), 
       auto.key = list(columns = 10,
                       reverse.rows = F))

##################################
# Pairwise association
# between descriptors when treated
# as quantitative variables
##################################
corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the maximally correlated variables
##################################
DPA_MaximallyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.80)
colnames(DPA.Descriptors.Numeric)[DPA_MaximallyCorrelated]

##################################
# Formulating the pairwise scatterplots
# among descriptors
##################################
splom(~DPA.Descriptors.Numeric,
      pch = 16,
      cex = 1,
      alpha = 0.45,
      auto.key = list(points = TRUE, space = "right"),
      main = "Exploratory Analysis Between Descriptors",
      xlab = "Scatterplot Matrix of Descriptors")

```

</details>

## 1.6 Factor Analysis

###  1.6.1 Principal Axes Factor Extraction and Varimax Rotation (FA_PA_V)
|
| [Principal Axes Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) identifies the underlying constructs that explain the observed correlations among variables by capturing both common variance (shared among variables) and unique variance (specific to each variable). This process potentially results to factors with lower communalities (explained variance) but with more direct interpretability. The algorithm performs eigenvalue decomposition on the correlation matrix. The eigenvalues represent the amount of variance explained by each eigenvector. Given a defined number of factors, loadings are calculated for each observed variable on each extracted factor. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Varimax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an orthogonal rotation method which forces the rotated factors to be uncorrelated with each other, leading to simpler and more easily interpretable factor solutions. The algorithm aims to maximize the variance of the squared loadings within each factor which helps identify variables that are strongly associated with a single factor. The results are straightforward to interpret and can be particularly useful when the factors are expectde to be independent.
|
| **[A]** Appplying **Principal Axes** factor extraction and **Varimax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** 4 factors would be sufficient for an optimal balance between comprehensiveness and parsimony. 
|      **[A.2]** To ensure that both under-extraction and over-extraction are assessed, models with 3, 4 and 5 factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|      **[A.3]** The choice of 4 factors was supported by maximum consensus (35.71%) from 5 (**Bentler**, **Beta**, **Parallel Analysis**, **Kaiser Criterion** and **Standardized Scree**) among 14 methods.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.07
|      **[B.2]** Tucker-Lewis Fit Index = 0.62
|      **[B.3]** Bayesian Information Criterion = -22.76
|      **[B.4]** High Residual Rate = 0.27
|      **[B.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.88, Communality = 0.79
|             **[B.5.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.79, Communality = 0.69
|             **[B.5.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.64, Communality = 0.54
|             **[B.5.4]** <span style="color: #FF0000">APPR</span>: Loading = 0.60, Communality = 0.50
|             **[B.5.5]** <span style="color: #FF0000">SCON</span>: Loading = 0.60, Communality = 0.51
|             **[B.5.6]** <span style="color: #FF0000">CFIT</span>: Loading = 0.60, Communality = 0.61
|             **[B.5.7]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">POTL</span>: Loading = 0.85, Communality = 0.87
|             **[B.6.2]** <span style="color: #FF0000">EXPR</span>: Loading = 0.76, Communality = 0.69
|             **[B.6.3]** <span style="color: #FF0000">ACAD</span>: Loading = 0.74, Communality = 0.65
|             **[B.6.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.56, Communality = 0.64
|             **[B.6.5]** Cronbach's Alpha = 0.88
|      **[B.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.88, Communality = 0.86
|             **[B.7.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.81, Communality = 0.83
|             **[B.7.3]** Cronbach's Alpha = 0.91
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.04
|      **[C.2]** Tucker-Lewis Fit Index = 0.73
|      **[C.3]** Bayesian Information Criterion = -33.02
|      **[C.4]** High Residual Rate = 0.15
|      **[C.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">POTL</span>: Loading = 0.85, Communality = 0.89
|             **[C.5.2]** <span style="color: #FF0000">EXPR</span>: Loading = 0.73, Communality = 0.68
|             **[C.5.3]** <span style="color: #FF0000">ACAD</span>: Loading = 0.71, Communality = 0.64
|             **[C.5.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.61, Communality = 0.73
|             **[C.5.5]** Cronbach's Alpha = 0.88
|      **[C.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.84, Communality = 0.86
|             **[C.6.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.76, Communality = 0.73
|             **[C.6.3]** <span style="color: #FF0000">CFIT</span>: Loading = 0.65, Communality = 0.76
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[C.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.82, Communality = 0.79
|             **[C.7.2]** <span style="color: #FF0000">APPR</span>: Loading = 0.70, Communality = 0.65
|             **[C.7.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.69, Communality = 0.66
|             **[C.7.4]** Cronbach's Alpha = 0.87
|      **[C.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.88, Communality = 0.89
|             **[C.8.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.83, Communality = 0.88
|             **[C.8.3]** Cronbach's Alpha = 0.91
|
| **[D]** Results for the exploratory factor analysis using a **5-Factor Structure** were as follows:
|      **[D.1]** Standardized Root Mean Square of the Residual = 0.02
|      **[D.2]** Tucker-Lewis Fit Index = 0.96
|      **[D.3]** Bayesian Information Criterion = -43.43
|      **[D.4]** High Residual Rate = 0.00
|      **[D.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[D.5.1]** <span style="color: #FF0000">ACAD</span>: Loading = 0.82, Communality = 0.79
|             **[D.5.2]** <span style="color: #FF0000">POTL</span>: Loading = 0.78, Communality = 0.87
|             **[D.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.70, Communality = 0.67
|             **[C.5.4]** Cronbach's Alpha = 0.89
|      **[D.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[D.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.86, Communality = 0.85
|             **[D.6.2]** <span style="color: #FF0000">LIKE</span>: Loading = 0.72, Communality = 0.69
|             **[D.6.3]** <span style="color: #FF0000">APPR</span>: Loading = 0.66, Communality = 0.64
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[D.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[D.7.1]** <span style="color: #FF0000">COMM</span>: Loading = 0.83, Communality = 0.86
|             **[D.7.2]** <span style="color: #FF0000">ORGN</span>: Loading = 0.83, Communality = 0.89
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[D.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[D.8.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.89, Communality = 0.89
|             **[D.8.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.82, Communality = 0.87
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[D.9]** Factor 5 was a latent variable with higher loading towards the following descriptors:
|             **[D.9.1]** <span style="color: #FF0000">JFIT</span>: Loading = 0.80, Communality = 0.93
|             **[D.9.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.74, Communality = 0.86
|             **[C.9.3]** Cronbach's Alpha = 0.94
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.1, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_PA_V_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "pa",
                                                           rotation = "varimax"))

as.data.frame(FA_PA_V_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 3 factors
##################################
(FA_PA_V_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_3F_Summary <- FA_PA_V_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_3F_Residual <- residuals(FA_PA_V_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_3F_RMS <- FA_PA_V_3F$rms)
(FA_PA_V_3F_TLI <- FA_PA_V_3F$TLI)
(FA_PA_V_3F_BIC <- FA_PA_V_3F$BIC)

(FA_PA_V_3F_MaxResidual   <- max(abs(FA_PA_V_3F_Residual),na.rm=TRUE))
(FA_PA_V_3F_HighResidual  <- sum(FA_PA_V_3F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_V_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_V_3F_HighResidualRate <- FA_PA_V_3F_HighResidual/FA_PA_V_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","LIKE","APPR","SCON","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("POTL","EXPR","ACAD","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 3,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 4 factors
##################################
(FA_PA_V_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_4F_Summary <- FA_PA_V_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_4F_Residual <- residuals(FA_PA_V_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_4F_RMS <- FA_PA_V_4F$rms)
(FA_PA_V_4F_TLI <- FA_PA_V_4F$TLI)
(FA_PA_V_4F_BIC <- FA_PA_V_4F$BIC)

(FA_PA_V_4F_MaxResidual   <- max(abs(FA_PA_V_4F_Residual),na.rm=TRUE))
(FA_PA_V_4F_HighResidual  <- sum(FA_PA_V_4F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_V_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_V_4F_HighResidualRate <- FA_PA_V_4F_HighResidual/FA_PA_V_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("POTL","EXPR","ACAD","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("SCON","APPR","LIKE")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 4,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 5 factors
##################################
(FA_PA_V_5F <- fa(DPA.Descriptors.Numeric,
              nfactors = 5,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_5F_Summary <- FA_PA_V_5F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_5F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_5F_Residual <- residuals(FA_PA_V_5F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_5F_RMS <- FA_PA_V_5F$rms)
(FA_PA_V_5F_TLI <- FA_PA_V_5F$TLI)
(FA_PA_V_5F_BIC <- FA_PA_V_5F$BIC)

(FA_PA_V_5F_MaxResidual   <- max(abs(FA_PA_V_5F_Residual),na.rm=TRUE))
(FA_PA_V_5F_HighResidual  <- sum(FA_PA_V_5F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_V_5F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_V_5F_HighResidualRate <- FA_PA_V_5F_HighResidual/FA_PA_V_5F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 5 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ACAD","POTL","EXPR")])
alpha(DPA.Descriptors.Numeric[,c("SCON","LIKE","APPR")])
alpha(DPA.Descriptors.Numeric[,c("COMM","ORGN")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_5F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 5,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_5F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

par(mfrow=c(1,3))
fa.diagram(FA_PA_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)
fa.diagram(FA_PA_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)
fa.diagram(FA_PA_V_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 5 Factors",
           cex=0.75)

```

</details>

###  1.6.2 Principal Axes Factor Extraction and Promax Rotation (FA_PA_P)
|
| [Principal Axes Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) identifies the underlying constructs that explain the observed correlations among variables by capturing both common variance (shared among variables) and unique variance (specific to each variable). This process potentially results to factors with lower communalities (explained variance) but with more direct interpretability. The algorithm performs eigenvalue decomposition on the correlation matrix. The eigenvalues represent the amount of variance explained by each eigenvector. Given a defined number of factors, loadings are calculated for each observed variable on each extracted factor. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Promax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an oblique rotation method which allows for more flexibility by accommodating the possibility of correlated factors. The algorithm aims to simplify the factor structure by both maximizing the variance of the squared loadings within each factor and allowing for correlated factors. It uses a more complex mathematical approach to find the optimal rotation that accounts for both variance and correlation. The results provide a more accurate representation of the underlying relationships when the factors are expected to be correlated.
|
| **[A]** Appplying **Principal Axes** factor extraction and **Promax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** 4 factors would be sufficient for an optimal balance between comprehensiveness and parsimony. 
|      **[A.2]** To ensure that both under-extraction and over-extraction are assessed, models with 3, 4 and 5 factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|      **[A.3]** The choice of 4 factors was supported by maximum consensus (35.71%) from 5 (**Bentler**, **Beta**, **Parallel Analysis**, **Kaiser Criterion** and **Standardized Scree**) among 14 methods.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.07
|      **[B.2]** Tucker-Lewis Fit Index = 0.62
|      **[B.3]** Bayesian Information Criterion = -22.76
|      **[B.4]** High Residual Rate = 0.27
|      **[B.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">ORGN</span>: Loading = 1.04, Communality = 0.79
|             **[B.5.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.91, Communality = 0.69
|             **[B.5.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.64, Communality = 0.54
|             **[B.5.4]** <span style="color: #FF0000">APPR</span>: Loading = 0.59, Communality = 0.50
|             **[B.5.5]** <span style="color: #FF0000">SCON</span>: Loading = 0.57, Communality = 0.51
|             **[B.5.6]** <span style="color: #FF0000">CFIT</span>: Loading = 0.52, Communality = 0.61
|             **[B.5.7]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">POTL</span>: Loading = 0.95, Communality = 0.87
|             **[B.6.2]** <span style="color: #FF0000">EXPR</span>: Loading = 0.88, Communality = 0.69
|             **[B.6.3]** <span style="color: #FF0000">ACAD</span>: Loading = 0.82, Communality = 0.65
|             **[B.6.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.48, Communality = 0.64
|             **[B.6.5]** Cronbach's Alpha = 0.88
|      **[B.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.94, Communality = 0.86
|             **[B.7.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.82, Communality = 0.83
|             **[B.7.3]** Cronbach's Alpha = 0.91
|      **[B.8]** Correlation between factors ranged from 0.48 to 0.64
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.04
|      **[C.2]** Tucker-Lewis Fit Index = 0.73
|      **[C.3]** Bayesian Information Criterion = -33.02
|      **[C.4]** High Residual Rate = 0.15
|      **[C.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">POTL</span>: Loading = 0.94, Communality = 0.89
|             **[C.5.2]** <span style="color: #FF0000">EXPR</span>: Loading = 0.81, Communality = 0.68
|             **[C.5.3]** <span style="color: #FF0000">ACAD</span>: Loading = 0.77, Communality = 0.64
|             **[C.5.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.57, Communality = 0.73
|             **[C.5.5]** Cronbach's Alpha = 0.88
|      **[C.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.92, Communality = 0.86
|             **[C.6.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.82, Communality = 0.73
|             **[C.6.3]** <span style="color: #FF0000">CFIT</span>: Loading = 0.64, Communality = 0.76
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[C.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.87, Communality = 0.79
|             **[C.7.2]** <span style="color: #FF0000">APPR</span>: Loading = 0.72, Communality = 0.65
|             **[C.7.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.68, Communality = 0.66
|             **[C.7.4]** Cronbach's Alpha = 0.87
|      **[C.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.95, Communality = 0.89
|             **[C.8.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.85, Communality = 0.88
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[C.9]** Correlation between factors ranged from 0.41 to 0.56
|
| **[D]** Results for the exploratory factor analysis using a **5-Factor Structure** were as follows:
|      **[D.1]** Standardized Root Mean Square of the Residual = 0.02
|      **[D.2]** Tucker-Lewis Fit Index = 0.96
|      **[D.3]** Bayesian Information Criterion = -43.43
|      **[D.4]** High Residual Rate = 0.00
|      **[D.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[D.5.1]** <span style="color: #FF0000">ACAD</span>: Loading = 0.95, Communality = 0.79
|             **[D.5.2]** <span style="color: #FF0000">POTL</span>: Loading = 0.80, Communality = 0.87
|             **[D.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.73, Communality = 0.67
|             **[C.5.4]** Cronbach's Alpha = 0.89
|      **[D.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[D.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 1.00, Communality = 0.85
|             **[D.6.2]** <span style="color: #FF0000">LIKE</span>: Loading = 0.78, Communality = 0.69
|             **[D.6.3]** <span style="color: #FF0000">APPR</span>: Loading = 0.67, Communality = 0.64
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[D.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[D.7.1]** <span style="color: #FF0000">COMM</span>: Loading = 0.91, Communality = 0.86
|             **[D.7.2]** <span style="color: #FF0000">ORGN</span>: Loading = 0.87, Communality = 0.89
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[D.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[D.8.1]** <span style="color: #FF0000">LETT</span>: Loading = 0.91, Communality = 0.89
|             **[D.8.2]** <span style="color: #FF0000">RESM</span>: Loading = 0.81, Communality = 0.87
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[D.9]** Factor 5 was a latent variable with higher loading towards the following descriptors:
|             **[D.9.1]** <span style="color: #FF0000">JFIT</span>: Loading = 0.96, Communality = 0.93
|             **[D.9.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.85, Communality = 0.86
|             **[C.9.3]** Cronbach's Alpha = 0.94
|      **[D.10]** Correlation between factors ranged from 0.36 to 0.61
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.2, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_PA_P_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "pa",
                                                           rotation = "promax"))

as.data.frame(FA_PA_P_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
# with 3 factors
##################################
(FA_PA_P_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_P_3F_Summary <- FA_PA_P_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_P_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_3F_Residual <- residuals(FA_PA_P_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_P_3F_RMS <- FA_PA_P_3F$rms)
(FA_PA_P_3F_TLI <- FA_PA_P_3F$TLI)
(FA_PA_P_3F_BIC <- FA_PA_P_3F$BIC)

(FA_PA_P_3F_MaxResidual   <- max(abs(FA_PA_P_3F_Residual),na.rm=TRUE))
(FA_PA_P_3F_HighResidual  <- sum(FA_PA_P_3F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_P_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_P_3F_HighResidualRate <- FA_PA_P_3F_HighResidual/FA_PA_P_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","LIKE","APPR","SCON","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("POTL","EXPR","ACAD","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 3,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
# with 4 factors
##################################
(FA_PA_P_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_P_4F_Summary <- FA_PA_P_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_P_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_4F_Residual <- residuals(FA_PA_P_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_P_4F_RMS <- FA_PA_P_4F$rms)
(FA_PA_P_4F_TLI <- FA_PA_P_4F$TLI)
(FA_PA_P_4F_BIC <- FA_PA_P_4F$BIC)

(FA_PA_P_4F_MaxResidual   <- max(abs(FA_PA_P_4F_Residual),na.rm=TRUE))
(FA_PA_P_4F_HighResidual  <- sum(FA_PA_P_4F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_P_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_P_4F_HighResidualRate <- FA_PA_P_4F_HighResidual/FA_PA_P_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("POTL","EXPR","ACAD","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("SCON","APPR","LIKE")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 4,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
# with 5 factors
##################################
(FA_PA_P_5F <- fa(DPA.Descriptors.Numeric,
              nfactors = 5,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_P_5F_Summary <- FA_PA_P_5F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_P_5F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_5F_Residual <- residuals(FA_PA_P_5F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_P_5F_RMS <- FA_PA_P_5F$rms)
(FA_PA_P_5F_TLI <- FA_PA_P_5F$TLI)
(FA_PA_P_5F_BIC <- FA_PA_P_5F$BIC)

(FA_PA_P_5F_MaxResidual   <- max(abs(FA_PA_P_5F_Residual),na.rm=TRUE))
(FA_PA_P_5F_HighResidual  <- sum(FA_PA_P_5F_Residual>abs(0.05),na.rm=TRUE))
(FA_PA_P_5F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_PA_P_5F_HighResidualRate <- FA_PA_P_5F_HighResidual/FA_PA_P_5F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 5 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ACAD","POTL","EXPR")])
alpha(DPA.Descriptors.Numeric[,c("SCON","LIKE","APPR")])
alpha(DPA.Descriptors.Numeric[,c("COMM","ORGN")])
alpha(DPA.Descriptors.Numeric[,c("LETT","RESM")])
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_5F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 5,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_5F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

par(mfrow=c(1,3))
fa.diagram(FA_PA_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)
fa.diagram(FA_PA_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)
fa.diagram(FA_PA_P_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 5 Factors",
           cex=0.75)

```

</details>

###  1.6.3 Maximum Likelihood Factor Extraction and Varimax Rotation (FA_ML_V)
|
| [Maximum Likelihood Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) aims to estimate the factor loadings in a way that maximizes the likelihood of observing the given data, assuming a specific factor model. Given the correlation matrix, the algorithm formulates a likelihood function that represents the probability of observing the given data under an assumed factor model representing the relationships between the latent factors and observed variables. The likelihood function quantifies how well the model explains the observed data. Optimization techniques are applied to determine the factor loadings that maximize the likelihood function. The process involves iteratively adjusting the factor loadings to improve the fit between the model and the data. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Varimax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an orthogonal rotation method which forces the rotated factors to be uncorrelated with each other, leading to simpler and more easily interpretable factor solutions. The algorithm aims to maximize the variance of the squared loadings within each factor which helps identify variables that are strongly associated with a single factor. The results are straightforward to interpret and can be particularly useful when the factors are expectde to be independent.
|
| **[A]** Appplying **Maximum Likelihood** factor extraction and **Varimax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** 4 factors would be sufficient for an optimal balance between comprehensiveness and parsimony. 
|      **[A.2]** To ensure that both under-extraction and over-extraction are assessed, models with 3, 4 and 5 factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|      **[A.3]** The choice of 4 factors was supported by maximum consensus (26.32%) from 5 (**Bentler**, **Beta**, **Parallel Analysis**, **Kaiser Criterion** and **Standardized Scree**) among 19 methods.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.08
|      **[B.2]** Tucker-Lewis Fit Index = 0.69
|      **[B.3]** Bayesian Information Criterion = -35.60
|      **[B.4]** High Residual Rate = 0.24
|      **[B.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">POTL</span>: Loading = 0.90, Communality = 0.92
|             **[B.5.2]** <span style="color: #FF0000">ACAD</span>: Loading = 0.76, Communality = 0.67
|             **[B.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.71, Communality = 0.61
|             **[B.5.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.62, Communality = 0.62
|             **[B.5.5]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.96, Communality = 0.96
|             **[B.6.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.85, Communality = 0.80
|             **[B.6.3]** <span style="color: #FF0000">CFIT</span>: Loading = 0.56, Communality = 0.62
|             **[B.6.4]** <span style="color: #FF0000">LIKE</span>: Loading = 0.48, Communality = 0.44
|             **[B.6.5]** <span style="color: #FF0000">APPR</span>: Loading = 0.48, Communality = 0.46
|             **[B.6.6]** <span style="color: #FF0000">SCON</span>: Loading = 0.43, Communality = 0.51
|             **[B.6.7]** Cronbach's Alpha = 0.88
|      **[B.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">RESM</span>: Loading = 0.93, Communality = 0.83
|             **[B.7.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.78, Communality = 0.71
|             **[B.7.3]** Cronbach's Alpha = 0.91
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.06
|      **[C.2]** Tucker-Lewis Fit Index = 0.76
|      **[C.3]** Bayesian Information Criterion = -37.51
|      **[C.4]** High Residual Rate = 0.15
|      **[C.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">JFIT</span>: Loading = 0.87, Communality = 0.91
|             **[C.5.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.78, Communality = 0.85
|             **[C.5.3]** <span style="color: #FF0000">POTL</span>: Loading = 0.66, Communality = 0.65
|             **[C.5.4]** <span style="color: #FF0000">EXPR</span>: Loading = 0.50, Communality = 0.46
|             **[C.5.5]** <span style="color: #FF0000">ACAD</span>: Loading = 0.49, Communality = 0.45
|             **[C.5.6]** Cronbach's Alpha = 0.90
|      **[C.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.81, Communality = 0.78
|             **[C.6.2]** <span style="color: #FF0000">APPR</span>: Loading = 0.72, Communality = 0.68
|             **[C.6.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.67, Communality = 0.65
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[C.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.91, Communality = 0.97
|             **[C.7.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.79, Communality = 0.79
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[C.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">RESM</span>: Loading = 0.91, Communality = 1.00
|             **[C.8.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.81, Communality = 0.78
|             **[C.8.3]** Cronbach's Alpha = 0.91
|
| **[D]** Results for the exploratory factor analysis using a **5-Factor Structure** were as follows:
|      **[D.1]** Standardized Root Mean Square of the Residual = 0.02
|      **[D.2]** Tucker-Lewis Fit Index = 0.99
|      **[D.3]** Bayesian Information Criterion = -45.32
|      **[D.4]** High Residual Rate = 0.03
|      **[D.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[D.5.1]** <span style="color: #FF0000">ACAD</span>: Loading = 0.81, Communality = 0.78
|             **[D.5.2]** <span style="color: #FF0000">POTL</span>: Loading = 0.79, Communality = 0.87
|             **[D.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.69, Communality = 0.64
|             **[C.5.4]** Cronbach's Alpha = 0.89
|      **[D.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[D.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.87, Communality = 0.87
|             **[D.6.2]** <span style="color: #FF0000">LIKE</span>: Loading = 0.71, Communality = 0.68
|             **[D.6.3]** <span style="color: #FF0000">APPR</span>: Loading = 0.65, Communality = 0.65
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[D.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[D.7.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.89, Communality = 0.96
|             **[D.7.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.79, Communality = 0.80
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[D.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[D.8.1]** <span style="color: #FF0000">RESM</span>: Loading = 0.90, Communality = 1.00
|             **[D.8.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.81, Communality = 0.77
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[D.9]** Factor 5 was a latent variable with higher loading towards the following descriptors:
|             **[D.9.1]** <span style="color: #FF0000">JFIT</span>: Loading = 0.81, Communality = 0.95
|             **[D.9.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.72, Communality = 0.85
|             **[C.9.3]** Cronbach's Alpha = 0.94
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.3, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_ML_V_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "mle",
                                                           rotation = "varimax"))

as.data.frame(FA_ML_V_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Varimax rotation
# with 3 factors
##################################
(FA_ML_V_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_V_3F_Summary <- FA_ML_V_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_V_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_3F_Residual <- residuals(FA_ML_V_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_V_3F_RMS <- FA_ML_V_3F$rms)
(FA_ML_V_3F_TLI <- FA_ML_V_3F$TLI)
(FA_ML_V_3F_BIC <- FA_ML_V_3F$BIC)

(FA_ML_V_3F_MaxResidual   <- max(abs(FA_ML_V_3F_Residual),na.rm=TRUE))
(FA_ML_V_3F_HighResidual  <- sum(FA_ML_V_3F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_V_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_V_3F_HighResidualRate <- FA_ML_V_3F_HighResidual/FA_ML_V_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("POTL","ACAD","EXPR","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","LIKE","APPR","SCON","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 3,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Varimax rotation
# with 4 factors
##################################
(FA_ML_V_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_V_4F_Summary <- FA_ML_V_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_V_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_4F_Residual <- residuals(FA_ML_V_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_V_4F_RMS <- FA_ML_V_4F$rms)
(FA_ML_V_4F_TLI <- FA_ML_V_4F$TLI)
(FA_ML_V_4F_BIC <- FA_ML_V_4F$BIC)

(FA_ML_V_4F_MaxResidual   <- max(abs(FA_ML_V_4F_Residual),na.rm=TRUE))
(FA_ML_V_4F_HighResidual  <- sum(FA_ML_V_4F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_V_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_V_4F_HighResidualRate <- FA_ML_V_4F_HighResidual/FA_ML_V_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT","POTL","EXPR","ACAD")])
alpha(DPA.Descriptors.Numeric[,c("SCON","APPR","LIKE")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 4,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Varimax rotation
# with 5 factors
##################################
(FA_ML_V_5F <- fa(DPA.Descriptors.Numeric,
              nfactors = 5,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_V_5F_Summary <- FA_ML_V_5F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_V_5F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_5F_Residual <- residuals(FA_ML_V_5F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_V_5F_RMS <- FA_ML_V_5F$rms)
(FA_ML_V_5F_TLI <- FA_ML_V_5F$TLI)
(FA_ML_V_5F_BIC <- FA_ML_V_5F$BIC)

(FA_ML_V_5F_MaxResidual   <- max(abs(FA_ML_V_5F_Residual),na.rm=TRUE))
(FA_ML_V_5F_HighResidual  <- sum(FA_ML_V_5F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_V_5F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_V_5F_HighResidualRate <- FA_ML_V_5F_HighResidual/FA_ML_V_5F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 5 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ACAD","POTL","EXPR")])
alpha(DPA.Descriptors.Numeric[,c("SCON","LIKE","APPR")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_5F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 5,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_5F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

par(mfrow=c(1,3))
fa.diagram(FA_ML_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)
fa.diagram(FA_ML_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)
fa.diagram(FA_ML_V_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 5 Factors",
           cex=0.75)

```

</details>

###  1.6.4 Maximum Likelihood Factor Extraction and Promax Rotation (FA_ML_P)
|
| [Maximum Likelihood Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) aims to estimate the factor loadings in a way that maximizes the likelihood of observing the given data, assuming a specific factor model. Given the correlation matrix, the algorithm formulates a likelihood function that represents the probability of observing the given data under an assumed factor model representing the relationships between the latent factors and observed variables. The likelihood function quantifies how well the model explains the observed data. Optimization techniques are applied to determine the factor loadings that maximize the likelihood function. The process involves iteratively adjusting the factor loadings to improve the fit between the model and the data. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Promax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an oblique rotation method which allows for more flexibility by accommodating the possibility of correlated factors. The algorithm aims to simplify the factor structure by both maximizing the variance of the squared loadings within each factor and allowing for correlated factors. It uses a more complex mathematical approach to find the optimal rotation that accounts for both variance and correlation. The results provide a more accurate representation of the underlying relationships when the factors are expected to be correlated.
|
| **[A]** Appplying **Maximum Likelihood** factor extraction and **Promax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** 4 factors would be sufficient for an optimal balance between comprehensiveness and parsimony. 
|      **[A.2]** To ensure that both under-extraction and over-extraction are assessed, models with 3, 4 and 5 factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|      **[A.3]** The choice of 4 factors was supported by maximum consensus (26.32%) from 5 (**Bentler**, **Beta**, **Parallel Analysis**, **Kaiser Criterion** and **Standardized Scree**) among 19 methods.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.08
|      **[B.2]** Tucker-Lewis Fit Index = 0.69
|      **[B.3]** Bayesian Information Criterion = -35.60
|      **[B.4]** High Residual Rate = 0.24
|      **[B.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">POTL</span>: Loading = 1.08, Communality = 0.92
|             **[B.5.2]** <span style="color: #FF0000">ACAD</span>: Loading = 0.88, Communality = 0.67
|             **[B.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.85, Communality = 0.61
|             **[B.5.4]** <span style="color: #FF0000">JFIT</span>: Loading = 0.60, Communality = 0.62
|             **[B.5.5]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">ORGN</span>: Loading = 1.18, Communality = 0.96
|             **[B.6.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.99, Communality = 0.80
|             **[B.6.3]** <span style="color: #FF0000">CFIT</span>: Loading = 0.47, Communality = 0.62
|             **[B.6.4]** <span style="color: #FF0000">LIKE</span>: Loading = 0.41, Communality = 0.44
|             **[B.6.5]** <span style="color: #FF0000">APPR</span>: Loading = 0.40, Communality = 0.46
|             **[B.6.6]** <span style="color: #FF0000">SCON</span>: Loading = 0.35, Communality = 0.51
|             **[B.6.7]** Cronbach's Alpha = 0.88
|      **[B.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">RESM</span>: Loading = 1.06, Communality = 0.83
|             **[B.7.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.87, Communality = 0.71
|             **[B.7.3]** Cronbach's Alpha = 0.91
|      **[B.8]** Correlation between factors ranged from 0.58 to 0.67
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.06
|      **[C.2]** Tucker-Lewis Fit Index = 0.76
|      **[C.3]** Bayesian Information Criterion = -37.51
|      **[C.4]** High Residual Rate = 0.15
|      **[C.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">JFIT</span>: Loading = 1.04, Communality = 0.91
|             **[C.5.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.91, Communality = 0.85
|             **[C.5.3]** <span style="color: #FF0000">POTL</span>: Loading = 0.70, Communality = 0.65
|             **[C.5.4]** <span style="color: #FF0000">EXPR</span>: Loading = 0.47, Communality = 0.46
|             **[C.5.5]** <span style="color: #FF0000">ACAD</span>: Loading = 0.44, Communality = 0.45
|             **[C.5.6]** Cronbach's Alpha = 0.90
|      **[C.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 0.93, Communality = 0.78
|             **[C.6.2]** <span style="color: #FF0000">APPR</span>: Loading = 0.80, Communality = 0.68
|             **[C.6.3]** <span style="color: #FF0000">LIKE</span>: Loading = 0.70, Communality = 0.65
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[C.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.99, Communality = 0.97
|             **[C.7.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.90, Communality = 0.79
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[C.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">RESM</span>: Loading = 0.89, Communality = 1.00
|             **[C.8.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.76, Communality = 0.78
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[C.9]** Correlation between factors ranged from 0.27 to 0.66
|
| **[D]** Results for the exploratory factor analysis using a **5-Factor Structure** were as follows:
|      **[D.1]** Standardized Root Mean Square of the Residual = 0.02
|      **[D.2]** Tucker-Lewis Fit Index = 0.99
|      **[D.3]** Bayesian Information Criterion = -45.32
|      **[D.4]** High Residual Rate = 0.03
|      **[D.5]** Factor 1 was a latent variable with higher loading towards the following descriptors:
|             **[D.5.1]** <span style="color: #FF0000">ACAD</span>: Loading = 0.95, Communality = 0.78
|             **[D.5.2]** <span style="color: #FF0000">POTL</span>: Loading = 0.82, Communality = 0.87
|             **[D.5.3]** <span style="color: #FF0000">EXPR</span>: Loading = 0.73, Communality = 0.64
|             **[C.5.4]** Cronbach's Alpha = 0.89
|      **[D.6]** Factor 2 was a latent variable with higher loading towards the following descriptors:
|             **[D.6.1]** <span style="color: #FF0000">SCON</span>: Loading = 1.02, Communality = 0.87
|             **[D.6.2]** <span style="color: #FF0000">LIKE</span>: Loading = 0.76, Communality = 0.68
|             **[D.6.3]** <span style="color: #FF0000">APPR</span>: Loading = 0.64, Communality = 0.65
|             **[C.6.4]** Cronbach's Alpha = 0.87
|      **[D.7]** Factor 3 was a latent variable with higher loading towards the following descriptors:
|             **[D.7.1]** <span style="color: #FF0000">ORGN</span>: Loading = 0.96, Communality = 0.96
|             **[D.7.2]** <span style="color: #FF0000">COMM</span>: Loading = 0.85, Communality = 0.80
|             **[C.7.3]** Cronbach's Alpha = 0.92
|      **[D.8]** Factor 4 was a latent variable with higher loading towards the following descriptors:
|             **[D.8.1]** <span style="color: #FF0000">RESM</span>: Loading = 0.93, Communality = 1.00
|             **[D.8.2]** <span style="color: #FF0000">LETT</span>: Loading = 0.79, Communality = 0.77
|             **[C.8.3]** Cronbach's Alpha = 0.91
|      **[D.9]** Factor 5 was a latent variable with higher loading towards the following descriptors:
|             **[D.9.1]** <span style="color: #FF0000">JFIT</span>: Loading = 0.96, Communality = 0.95
|             **[D.9.2]** <span style="color: #FF0000">CFIT</span>: Loading = 0.87, Communality = 0.85
|             **[C.9.3]** Cronbach's Alpha = 0.94
|      **[D.10]** Correlation between factors ranged from 0.39 to 0.62
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.4, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_ML_P_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "mle",
                                                           rotation = "promax"))

as.data.frame(FA_ML_P_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Promax rotation
# with 3 factors
##################################
(FA_ML_P_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_P_3F_Summary <- FA_ML_P_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_P_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_3F_Residual <- residuals(FA_ML_P_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_P_3F_RMS <- FA_ML_P_3F$rms)
(FA_ML_P_3F_TLI <- FA_ML_P_3F$TLI)
(FA_ML_P_3F_BIC <- FA_ML_P_3F$BIC)

(FA_ML_P_3F_MaxResidual   <- max(abs(FA_ML_P_3F_Residual),na.rm=TRUE))
(FA_ML_P_3F_HighResidual  <- sum(FA_ML_P_3F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_P_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_P_3F_HighResidualRate <- FA_ML_P_3F_HighResidual/FA_ML_P_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("POTL","ACAD","EXPR","JFIT")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM","LIKE","APPR","SCON","CFIT")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_P_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 3,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Promax rotation
# with 4 factors
##################################
(FA_ML_P_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_P_4F_Summary <- FA_ML_P_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_P_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_4F_Residual <- residuals(FA_ML_P_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_P_4F_RMS <- FA_ML_P_4F$rms)
(FA_ML_P_4F_TLI <- FA_ML_P_4F$TLI)
(FA_ML_P_4F_BIC <- FA_ML_P_4F$BIC)

(FA_ML_P_4F_MaxResidual   <- max(abs(FA_ML_P_4F_Residual),na.rm=TRUE))
(FA_ML_P_4F_HighResidual  <- sum(FA_ML_P_4F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_P_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_P_4F_HighResidualRate <- FA_ML_P_4F_HighResidual/FA_ML_P_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT","POTL","EXPR","ACAD")])
alpha(DPA.Descriptors.Numeric[,c("SCON","APPR","LIKE")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_P_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 4,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Promax rotation
# with 5 factors
##################################
(FA_ML_P_5F <- fa(DPA.Descriptors.Numeric,
              nfactors = 5,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_P_5F_Summary <- FA_ML_P_5F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_P_5F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_5F_Residual <- residuals(FA_ML_P_5F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_P_5F_RMS <- FA_ML_P_5F$rms)
(FA_ML_P_5F_TLI <- FA_ML_P_5F$TLI)
(FA_ML_P_5F_BIC <- FA_ML_P_5F$BIC)

(FA_ML_P_5F_MaxResidual   <- max(abs(FA_ML_P_5F_Residual),na.rm=TRUE))
(FA_ML_P_5F_HighResidual  <- sum(FA_ML_P_5F_Residual>abs(0.05),na.rm=TRUE))
(FA_ML_P_5F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)
(FA_ML_P_5F_HighResidualRate <- FA_ML_P_5F_HighResidual/FA_ML_P_5F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_P_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 5 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("ACAD","POTL","EXPR")])
alpha(DPA.Descriptors.Numeric[,c("SCON","LIKE","APPR")])
alpha(DPA.Descriptors.Numeric[,c("ORGN","COMM")])
alpha(DPA.Descriptors.Numeric[,c("RESM","LETT")])
alpha(DPA.Descriptors.Numeric[,c("JFIT","CFIT")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_P_5F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 5,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_5F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

par(mfrow=c(1,3))
fa.diagram(FA_ML_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)
fa.diagram(FA_ML_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)
fa.diagram(FA_ML_P_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 5 Factors",
           cex=0.75)

```

</details>


##  1.7 Algorithm Comparison Summary
|
| **[A]** Among candidates, optimal results were obtained for the exploratory factor analysis model with **5-Factor Structure** by demonstrating excellent model fit metrics. In addition, the latent variables obtained were contextually meaningful as individual factors in the analysis.
|      **[A.1]** Lowest Standardized Root Mean Square of the Residual
|      **[A.2]** Highest Tucker-Lewis Fit Index
|      **[A.3]** Lowest Bayesian Information Criterion
|      **[A.4]** Lowest High Residual Rate
|
| **[B]** Among candidates, minimal errors were obtained for the exploratory factor analysis model with **Principal Axes** factor extraction by demonstrating the:
|      **[A.1]** Lowest Standardized Root Mean Square of the Residual
|      **[A.2]** Lowest High Residual Rate
|
| **[C]** Among candidates, the exploratory factor analysis model with **Promax** rotation was more appropriate due to the presence of considerable correlation among the extracted factors.
|
| **[D]** The selected exploratory factor analysis model demonstrated highly consistent factors described as follows: 
|      **[D.1]** Factor 1 corresponds to the academic and professional background of the candidate.
|             **[D.1.1]** <span style="color: #FF0000">ACAD</span> (Academic Record)
|             **[D.1.2]** <span style="color: #FF0000">POTL</span> (Potential)
|             **[D.1.3]** <span style="color: #FF0000">EXPR</span> (Experience)
|      **[D.2]** Factor 2 corresponds to the candidate's personality.
|             **[D.2.1]** <span style="color: #FF0000">SCON</span> (Self-Confidence)
|             **[D.2.2]** <span style="color: #FF0000">LIKE</span> (Likeability)
|             **[D.2.3]** <span style="color: #FF0000">APPR</span> (Appearance)
|      **[D.3]** Factor 3 corresponds to the candidate's soft skills.
|             **[D.3.1]** <span style="color: #FF0000">COMM</span> (Communication)
|             **[D.3.2]** <span style="color: #FF0000">ORGN</span> (Organization)
|      **[D.4]** Factor 4 corresponds to the candidate's presentation of skills and qualifications during application. 
|             **[D.4.1]** <span style="color: #FF0000">LETT</span> (Cover Letter)
|             **[D.4.2]** <span style="color: #FF0000">RESM</span> (Resume)
|      **[D.5]** Factor 5 corresponds to the candidate's overall fit to the company and job.
|             **[D.5.1]** <span style="color: #FF0000">JFIT</span> (Job Fit)
|             **[D.5.2]** <span style="color: #FF0000">CFIT</span> (Company Fit)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6, warning=FALSE, message=FALSE, dev='png'}

##################################
# Consolidating the fit indices
##################################
FA_RMSSummary <- c(FA_PA_V_3F_RMS,
                   FA_PA_V_4F_RMS,
                   FA_PA_V_5F_RMS,
                   FA_PA_P_3F_RMS,
                   FA_PA_P_4F_RMS,
                   FA_PA_P_5F_RMS,
                   FA_ML_V_3F_RMS,
                   FA_ML_V_4F_RMS,
                   FA_ML_V_5F_RMS,
                   FA_ML_P_3F_RMS,
                   FA_ML_P_4F_RMS,
                   FA_ML_P_5F_RMS)

FA_TLISummary <- c(FA_PA_V_3F_TLI,
                   FA_PA_V_4F_TLI,
                   FA_PA_V_5F_TLI,
                   FA_PA_P_3F_TLI,
                   FA_PA_P_4F_TLI,
                   FA_PA_P_5F_TLI,
                   FA_ML_V_3F_TLI,
                   FA_ML_V_4F_TLI,
                   FA_ML_V_5F_TLI,
                   FA_ML_P_3F_TLI,
                   FA_ML_P_4F_TLI,
                   FA_ML_P_5F_TLI)

FA_BICSummary <- c(FA_PA_V_3F_BIC,
                   FA_PA_V_4F_BIC,
                   FA_PA_V_5F_BIC,
                   FA_PA_P_3F_BIC,
                   FA_PA_P_4F_BIC,
                   FA_PA_P_5F_BIC,
                   FA_ML_V_3F_BIC,
                   FA_ML_V_4F_BIC,
                   FA_ML_V_5F_BIC,
                   FA_ML_P_3F_BIC,
                   FA_ML_P_4F_BIC,
                   FA_ML_P_5F_BIC)

FA_HighResidualRateSummary <- c(FA_PA_V_3F_HighResidualRate,
                                FA_PA_V_4F_HighResidualRate,
                                FA_PA_V_5F_HighResidualRate,
                                FA_PA_P_3F_HighResidualRate,
                                FA_PA_P_4F_HighResidualRate,
                                FA_PA_P_5F_HighResidualRate,
                                FA_ML_V_3F_HighResidualRate,
                                FA_ML_V_4F_HighResidualRate,
                                FA_ML_V_5F_HighResidualRate,
                                FA_ML_P_3F_HighResidualRate,
                                FA_ML_P_4F_HighResidualRate,
                                FA_ML_P_5F_HighResidualRate)

FA_AlgorithmSummary <- c("FA_PA_V_3F",
                         "FA_PA_V_4F",
                         "FA_PA_V_5F",
                         "FA_PA_P_3F",
                         "FA_PA_P_4F",
                         "FA_PA_P_5F",
                         "FA_ML_V_3F",
                         "FA_ML_V_4F",
                         "FA_ML_V_5F",
                         "FA_ML_P_3F",
                         "FA_ML_P_4F",
                         "FA_ML_P_5F")

FA_Summary <- cbind(FA_RMSSummary,
                    FA_TLISummary,
                    FA_BICSummary,
                    FA_HighResidualRateSummary,
                    FA_AlgorithmSummary)

FA_Summary <- as.data.frame(FA_Summary)
names(FA_Summary) <- c("RMS",
                       "TLI",
                       "BIC",
                       "HighResidualRate",
                       "Algorithm")

FA_Summary$RMS <- as.numeric(as.character(FA_Summary$RMS))
FA_Summary$TLI <- as.numeric(as.character(FA_Summary$TLI))
FA_Summary$BIC <- as.numeric(as.character(FA_Summary$BIC))
FA_Summary$HighResidualRate <- as.numeric(as.character(FA_Summary$HighResidualRate))

FA_Summary$Algorithm <- factor(FA_Summary$Algorithm ,
                                        levels = c("FA_PA_V_3F",
                                                   "FA_PA_V_4F",
                                                   "FA_PA_V_5F",
                                                   "FA_PA_P_3F",
                                                   "FA_PA_P_4F",
                                                   "FA_PA_P_5F",
                                                   "FA_ML_V_3F",
                                                   "FA_ML_V_4F",
                                                   "FA_ML_V_5F",
                                                   "FA_ML_P_3F",
                                                   "FA_ML_P_4F",
                                                   "FA_ML_P_5F"))

print(FA_Summary, row.names=FALSE)

##################################
# Consolidating all calculated values
# for the Standardized Root Mean Square of the Residual
##################################
(RMS_Plot <- dotplot(Algorithm ~ RMS,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Standardized Root Mean Square of the Residual",
                     ylab = "Algorithm",
                     xlab = "RMSR",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Tucker-Lewis Fit Index
##################################
(TLI_Plot <- dotplot(Algorithm ~ TLI,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Tucker-Lewis Fit Index",
                     ylab = "Algorithm",
                     xlab = "TLI",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Bayesian Information Criterion
##################################
(BIC_Plot <- dotplot(Algorithm ~ BIC,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Bayesian Information Criterion",
                     ylab = "Algorithm",
                     xlab = "BIC",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the High Residual Rate
##################################
(HighResidualRate_Plot <- dotplot(Algorithm ~ HighResidualRate,
                     data = FA_Summary,
                     main = "Algorithm Comparison : High Residual Rate ",
                     ylab = "Algorithm",
                     xlab = "High Residual Rate",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Plotting the Factor Loading Diagram
# for the optimal EFA model
##################################
fa.diagram(FA_PA_P_5F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 5 Factors",
           cex=0.75)

##################################
# Plotting the Dandelion Plot
# for the optimal EFA model
##################################
dandelion(FA_PA_P_5F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

|
# **2. References**
|
| **[Book]** [Using Multivariate Analysis](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) by Barbara Tabachnick and Linda Fidell
| **[Book]** [A Step-by-Step Guide to Exploratory Factor Analysis with R and RStudio](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) by Marley Watkins
| **[Book]** [A Step-by-Step Guide to Exploratory Factor Analysis with R and RStudio](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) by Marley Watkins
| **[Book]** [Multiple Factor Analysis by Example Using R](https://www.oreilly.com/library/view/multiple-factor-analysis/9781498786690/) by Jerome Pages
| **[Book]** [Nonlinear Principal Component Analysis and Its Applications](https://link.springer.com/book/10.1007/978-981-10-0159-8#toc) by Yuichi Mori, Masahiro Kuroda and Naomichi Makino
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [tidyr](https://cran.r-project.org/web/packages/tidyr/tidyr.pdf) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [factoextra](https://cran.r-project.org/web/packages/factoextra/factoextra.pdf) by Alboukadel Kassambara and Fabian Mundt
| **[R Package]** [FactoMineR](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Francois Husson, Julie Josse, Sebastien Le and Jeremy Mazet
| **[R Package]** [gplots](https://cran.r-project.org/web/packages/gplots/gplots.pdf) by Tal Galili
| **[R Package]** [qgraph](https://cran.r-project.org/web/packages/qgraph/qgraph.pdf) by Sacha Epskamp
| **[R Package]** [ggplot2](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Hadley Wickham, Winston Chang, Lionel Henry and Thomas Lin Pedersen
| **[R Package]** [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) by William Revelle
| **[R Package]** [nFactors](https://cran.r-project.org/web/packages/nFactors/nFactors.pdf) by Gilles Raiche and David Magis
| **[R Package]** [MBESS](https://cran.r-project.org/web/packages/MBESS/MBESS.pdf) by Ken Kelley
| **[R Package]** [DandEFA](https://cran.r-project.org/web/packages/DandEFA/DandEFA.pdf) by Artur Manukyan, Ahmet Sedef, Erhan Cene and Ibrahim Demir
| **[R Package]** [EFAtools](https://cran.r-project.org/web/packages/EFAtools/EFAtools.pdf) by Markus Steiner and Silvia Grieder
| **[R Package]** [parameters](https://cran.r-project.org/web/packages/parameters/parameters.pdf) by Daniel Ludecke
| **[R Package]** [performance](https://cran.r-project.org/web/packages/performance/performance.pdf) by Daniel Ludecke
| **[R Package]** [HH](https://cran.r-project.org/web/packages/HH/HH.pdf) by Richard Heiberger
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Factor Analysis with the psych package](https://m-clark.github.io/posts/2020-04-10-psych-explained/) by Michael Clark
| **[Article]** [Factor Analysis in R with Psych Package: Measuring Consumer Involvement](https://www.r-bloggers.com/2019/01/factor-analysis-in-r-with-psych-package-measuring-consumer-involvement/) by Peter Prevos
| **[Article]** [Factor Analysis in R](http://jinjian-mu.com/tutorial/2021-04-14-Factor%20Analysis/) by Jinjian Mu
| **[Article]** [How To: Use the psych package for Factor Analysis and Data Reduction](http://personality-project.org/r/psych/HowTo/factor.pdf) by William Revelle
| **[Article]** [A Practical Introduction to Factor Analysis: Exploratory Factor Analysis](https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) by UCLA Advanced Research Computing Team
| **[Article]** [Examining the Big 5 Personality Dataset with Factor Analysis](https://taridwong.github.io/posts/2022-01-01-efacfa/) by Tarid Wongvorachan
| **[Article]** [Principal Component Analysis versus Exploratory Factor Analysis](https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/203-30.pdf) by Diana Suhr
| **[Article]** [Exploratory Factor Analysis](https://www.publichealth.columbia.edu/research/population-health-methods/exploratory-factor-analysis) by Columbia University Irving Medical Center
| **[Article]** [Factor Analysis Example](https://real-statistics.com/multivariate-statistics/factor-analysis/factor-analysis-example/) by Charles Zaiontz
| **[Article]** [Factor Analysis Guide with an Example](https://statisticsbyjim.com/basics/factor-analysis/) by Jim Frost
| **[Article]** [What Is Factor Analysis and How Does It Simplify Research Findings?](https://www.qualtrics.com/experience-management/research/factor-analysis/) by Qualtrics Team
| **[Article]** [How Can I Perform A Factor Analysis With Categorical (Or Categorical And Continuous) Variables?](https://stats.oarc.ucla.edu/stata/faq/how-can-i-perform-a-factor-analysis-with-categorical-or-categorical-and-continuous-variables/) by UCLA Advanced Research Computing Team
| **[Article]** [Factor Analysis on Ordinal Data Example in R (psych, homals)](https://alice86.github.io/2018/04/08/Factor-Analysis-on-Ordinal-Data-example-in-R-(psych,-homals)/) by Jiayu Wu
| **[Article]** [Factor Analysis](https://handwiki.org/wiki/Factor%20analysis) by HandWiki Team
| **[Article]** [On Likert Scales In R](https://jakec007.github.io/2021-06-23-R-likert/) by Jake Chanenson
| **[Publication]** [General Intelligence Objectively Determined and Measured](https://psycnet.apa.org/record/1926-00296-001) by Charles Spearman (The American Journal of Psychology)
| **[Publication]** [The Effect of Standardization on a Chi-Square Approximation in Factor Analysis](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) by Maurice Bartlett (Psychometrika)
| **[Publication]** [A Second Generation Little Jiffy](https://link.springer.com/article/10.1007/BF02291817) by Henry Kaiser (Psychometrika)
| **[Publication]** [Tests of Significance in Factor Analysis](https://www.semanticscholar.org/paper/TESTS-OF-SIGNIFICANCE-IN-FACTOR-ANALYSIS-Burt/25975f19d17ab2577845ec3d61f52806b28d8f28) by Maurice Bartlett (British Journal of Statistical Psychology)
| **[Publication]** [Test of Linear Trend in Eigenvalues of a Covariance Matrix with Application to Data Analysis](https://psycnet.apa.org/record/1996-01978-006) by Peter Bentler and KeHai Yuan (British Journal of Mathematical and Statistical Psychology)
| **[Publication]** [The Scree Test For The Number Of Factors](https://www.semanticscholar.org/paper/The-Scree-Test-For-The-Number-Of-Factors.-Cattell/379df72de684003963f11427c97490a8c2d2a593) by Raymond Cattell (Multivariate Behavioral Research)
| **[Publication]** [Using Fit Statistic Differences to Determine the Optimal Number of Factors to Retain in an Exploratory Factor Analysis](https://journals.sagepub.com/doi/10.1177/0013164419865769) by William Finch (Educational and Psychological Measurement)
| **[Publication]** [An Objective Counterpart to the Visual Scree Test for Factor Analysis: The Standard Error Scree](https://journals.sagepub.com/doi/10.1177/0013164496056003006) by Keith Zoski and Stephen Jurs (Educational and Psychological Measurement)
| **[Publication]** [The Performance of Regression-Based Variations of the Visual Scree for Determining the Number of Common Factors](https://journals.sagepub.com/doi/10.1177/00164402062003001) by Fadia Nasser, Jeri Benson and Joseph Wisenbaker (Educational and Psychological Measurement)
| **[Publication]** [Investigating the Performance of Exploratory Graph Analysis and Traditional Techniques to Identify the Number of Latent Factors: A Simulation and Tutorial](https://www.semanticscholar.org/paper/Investigating-the-performance-of-exploratory-graph-Golino-Shi/470c4e8aeebd08699fe9092463540a1b24b7e2e8) by Hudson Golino, Dingjing Shi, Alexander Christensen, Luis Garrido, Maria Nieto, Ritu Sadana, Jotheeswaran Thiyagarajan, Agustin Martinez-Molina (Psychological Methods)
| **[Publication]** [Exploratory Graph Analysis: A New Approach for Estimating the Number of Dimensions in Psychological Research](https://www.semanticscholar.org/paper/Exploratory-graph-analysis%3A-A-new-approach-for-the-Golino-Epskamp/f44110bff4345eb228b27de8a0b8aec235edd478) by Hudson Galino and Sacha Epskamp (Plos One)
| **[Publication]** [Very Simple Structure: An Alternative Procedure For Estimating The Optimal Number Of Interpretable Factors](https://www.tandfonline.com/doi/abs/10.1207/s15327906mbr1404_2) by William Revelle and Thomas Rocklin (Multivariate Behavioral Research )
| **[Publication]** [Determining the Number of Components from the Matrix of Partial Correlations](https://psycnet.apa.org/record/1977-07293-001) by Wayne Velicer (Psychometrika)
| **[Publication]** [Dandelion Plot: A Method for the Visualization of R-mode Exploratory Factor Analyses](https://link.springer.com/article/10.1007/s00180-014-0518-x) by Artur Manukyan, Erhan Cene, Ahmet Sedef and Ibrahim Demir (Computational Statistics) 
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|