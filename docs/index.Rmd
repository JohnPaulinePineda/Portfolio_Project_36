---
title: 'Unsupervised Learning : Discovering Latent Dimensions using Exploratory Factor Analysis with Extraction and Rotation Method Combinations'
author: "John Pauline Pineda"
date: "August 10, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```

# **1. Table of Contents**
|
|
##  1.1 Sample Data
|
| The [<mark style="background-color: #EEEEEE;color: #FF0000">**TeacherEvaluation**</mark>](https://real-statistics.com/multivariate-statistics/factor-analysis/factor-analysis-example/) dataset obtained from the [<mark style="background-color: #CCECFF">**Real Statistics Using Excel**</mark>](https://real-statistics.com/) website by [Charles Zaiontz](https://real-statistics.com/contact-us/) was used for this illustrated example.   
|
| Preliminary dataset assessment:
|
| **[A]** 120 rows (observations)
| 
| **[B]** 9 columns (variables)
|      **[B.1]** 9/9 descriptors = 9/9 ordered category 
|             **[B.1.1]** <span style="color: #FF0000">EXP</span> (Setting high expectations for the students)
|             **[B.1.2]** <span style="color: #FF0000">ENT</span> (Entertaining)
|             **[B.1.3]** <span style="color: #FF0000">COM</span> (Able to communicate effectively)
|             **[B.1.4]** <span style="color: #FF0000">MAS</span> (Having mastery in their subject)
|             **[B.1.5]** <span style="color: #FF0000">MOT</span> (Able to motivate)
|             **[B.1.6]** <span style="color: #FF0000">CAR</span> (Caring)
|             **[B.1.7]** <span style="color: #FF0000">CHA</span> (Charismatic)
|             **[B.1.8]** <span style="color: #FF0000">PAS</span> (Having a passion for teaching)
|             **[B.1.9]** <span style="color: #FF0000">FRI</span> (Friendly and easy-going)
|     
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(performance)
library(parameters)
library(caret)
library(psych)
library(lattice)
library(dplyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(factoextra)
library(FactoMineR)
library(gplots)
library(qgraph)
library(ggplot2)
library(psych)
library(nFactors)
library(MBESS)
library(DandEFA)
library(EFAtools)


##################################
# Loading source and
# formulating the analysis set
##################################
TeacherEvaluation <- read.csv("TeacherEvaluation.csv",
                   na.strings=c("NA","NaN"," ",""),
                   stringsAsFactors = FALSE)
TeacherEvaluation <- as.data.frame(TeacherEvaluation)

##################################
# Performing a general exploration of the data set
##################################
dim(TeacherEvaluation)
str(TeacherEvaluation)
summary(TeacherEvaluation)

##################################
# Formulating a data type assessment summary
##################################
PDA <- TeacherEvaluation
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

##  1.2 Data Quality Assessment
|
| Data quality assessment:
|
| **[A]** No missing observations noted for any variable.
|
| **[B]** No low variance observed for any variable with First.Second.Mode.Ratio>5.
|
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
|
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- TeacherEvaluation

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}
```

</details>

##  1.3 Data Preprocessing

###  1.3.1 Outlier Detection
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- TeacherEvaluation

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))

```

```{r section_1.3.1.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Outlier Detection
##################################

##################################
# Listing all Descriptors
##################################
DPA.Descriptors <- DPA

##################################
# Listing all numeric Descriptors
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors[,sapply(DPA.Descriptors, is.numeric)]

##################################
# Identifying outliers for the numeric Descriptors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Descriptors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Descriptors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}


```

</details>

###  1.3.2 Zero and Near-Zero Variance
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance descriptors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}

```

</details>

###  1.3.3 Collinearity
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3, warning=FALSE, message=FALSE}
##################################
# Visualizing pairwise correlation between descriptor
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "circle",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the minimally correlated variables
##################################
(DPA_MinimallyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.30))

qgraph(cor(DPA.Descriptors.Numeric),
       cut=0.30,
       details=TRUE,
       posCol="#2F75B5",
       negCol="#FF5050",
       labels=names(DPA.Descriptors.Numeric))

##################################
# Identifying the highly correlated variables
##################################
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.90))

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.90)

  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))

  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))

  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
  DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_HighlyCorrelated[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_HighlyCorrelatedRemovedVariable))
  }

}



```

</details>

###  1.3.4 Linear Dependency
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4, warning=FALSE, message=FALSE}
##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Predictors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Predictors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}

```

</details>

###  1.3.5 Distributional Shape
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Distributional Shape
##################################

##################################
# Formulating the histogram
# for the numeric descriptors
##################################
for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Median <- format(round(median(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA.Descriptors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Descriptors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

```

</details>

##  1.4 Data Pre-Assessment

###  1.4.1 Correlation Matrix Assessment - Determinant Computation
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.1, warning=FALSE, message=FALSE}

##################################
# Computing the determinant of the correlation matrix
##################################
(DPA_CorrelationMatrixDeterminant <- det(cor(DPA.Descriptors.Numeric)))

```

</details>

###  1.4.2 Correlation Matrix Assessment - Bartlettâ€™s Test of Sphericity
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.2, warning=FALSE, message=FALSE}
##################################
# Calculating the Bartlett's Test of Sphericity
##################################
(DPA_BartlettTest <- cortest.bartlett(DPA.Descriptors.Numeric,
                                      n=nrow(DPA.Descriptors.Numeric)))

```

</details>

###  1.4.3 Correlation Matrix Assessment - Kaiser-Meyer-Olkin Factor Adequacy
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.3, warning=FALSE, message=FALSE}
##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric[,!names(DPA.Descriptors.Numeric) %in% c("FRI")]))

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric[,!names(DPA.Descriptors.Numeric) %in% c("FRI","EXP")]))

DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!names(DPA.Descriptors.Numeric) %in% c("FRI","EXP")]


```

</details>

###  1.4.4 Factor Retention - Parallel Analysis
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.4.4, warning=FALSE, message=FALSE}
##################################
# Comparing the scree of factors
# of the observed data with that
# of a random data matrix
# of the same size as the original
##################################
set.seed(88888888)
(DPA_ParallelAnalysis <- fa.parallel(DPA.Descriptors.Numeric,
                                      fa="fa",
                                      n.iter=500,
                                      plot=FALSE))

(DPA_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric))

as.data.frame(DPA_MethodAgreementProcedure)

```

</details>

## 1.5 Data Exploration
|
| Exploratory data analysis:
|
| **[A]** Critic and audience scores including their gaps from both sources were correlated.
|      **[A.1]** High <span style="color: #FF0000">Tomatometer_Critic</span> = High <span style="color: #FF0000">IMDB_Critic</span>
|      **[A.2]** High <span style="color: #FF0000">Tomatometer_Audience</span> = High <span style="color: #FF0000">IMDB_Audience</span>
|      **[A.3]** Low <span style="color: #FF0000">Tomatometer_Critic_Audience_Gap</span> = Low <span style="color: #FF0000">IMDB_Critic_Audience_Gap</span>
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.5, warning=FALSE, message=FALSE}
##################################
# Exploratory analysis
# among quantitative descriptors
##################################
splom(~DPA.Descriptors.Numeric,
      pch = 16,
      cex = 1,
      alpha = 0.45,
      auto.key = list(points = TRUE, space = "right"),
      main = "Exploratory Analysis Between Descriptors",
      xlab = "Scatterplot Matrix of Descriptors")

```

</details>

## 1.6 Factor Analysis

###  1.6.1 Principal Axes Factor Extraction and Varimax Rotation (FA_PA_V)
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.1, warning=FALSE, message=FALSE}
##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
##################################
FA_PA_V <- fa(DPA.Descriptors.Numeric,
              nfactors = 2,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric))


(FA_PA_V_Summary <- FA_PA_V %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_Summary)

print(FA_PA_V,
      digits=2,
      sort=TRUE)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_Residual <- residuals(FA_PA_V,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V,
           sort=TRUE,
           cut=0,
           digits=3,
           main="FA_PA_V",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("CHA","COM","ENT")])
alpha(DPA.Descriptors.Numeric[,c("MOT","PAS","CAR","MAS")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 2,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

###  1.6.2 Principal Axes Factor Extraction and Promax Rotation (FA_PA_P)
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.2, warning=FALSE, message=FALSE}
##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
##################################
FA_PA_P <- fa(DPA.Descriptors.Numeric,
              nfactors = 2,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric))

FA_PA_P %>%
  model_parameters(sort = TRUE, threshold = "max")

print(FA_PA_P,
      digits=3,
      sort=TRUE)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_Residual <- residuals(FA_PA_P,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P,
           sort=TRUE,
           cut=0,
           digits=3,
           main="FA_PA_P",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("CHA","COM","ENT")])
alpha(DPA.Descriptors.Numeric[,c("MOT","CAR","PAS","MAS")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 2,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

###  1.6.3 Maximum Likelihood Factor Extraction and Varimax Rotation (FA_ML_V)
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.3, warning=FALSE, message=FALSE}
##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
##################################
FA_ML_V <- fa(DPA.Descriptors.Numeric,
              nfactors = 2,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric))


FA_ML_V %>%
  model_parameters(sort = TRUE, threshold = "max")

print(FA_ML_V,
      digits=3,
      sort=TRUE)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_Residual <- residuals(FA_ML_V,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V,
           sort=TRUE,
           cut=0,
           digits=3,
           main="FA_ML_V",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("CHA","COM","ENT")])
alpha(DPA.Descriptors.Numeric[,c("MOT","CAR","PAS","MAS")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 2,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

###  1.6.4 Maximum Likelihood Factor Extraction and Promax Rotation (FA_ML_P)
|
|
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6.4, warning=FALSE, message=FALSE}
##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
##################################
FA_ML_V <- fa(DPA.Descriptors.Numeric,
              nfactors = 2,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric))


FA_ML_V %>%
  model_parameters(sort = TRUE, threshold = "max")

print(FA_ML_V,
      digits=3,
      sort=TRUE)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_Residual <- residuals(FA_ML_V,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V,
           sort=TRUE,
           cut=0,
           digits=3,
           main="FA_ML_V",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("CHA","COM","ENT")])
alpha(DPA.Descriptors.Numeric[,c("MOT","CAR","PAS","MAS")])

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
(FA_ML_P_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 2,
                                  rotation = "promax"))

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>


##  1.7 Algorithm Comparison Summary
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.6, warning=FALSE, message=FALSE, dev='png'}



```

</details>

|
# **2. References**
|
| **[Book]** [Multiple Factor Analysis by Example Using R](https://www.oreilly.com/library/view/multiple-factor-analysis/9781498786690/) by Jerome Pages
| **[Book]** [Nonlinear Principal Component Analysis and Its Applications](https://link.springer.com/book/10.1007/978-981-10-0159-8#toc) by Yuichi Mori, Masahiro Kuroda and Naomichi Makino
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[Book]** [A Step-by-Step Guide to Exploratory Factor Analysis with R and RStudio](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) by Marley Watkins
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [factoextra](https://cran.r-project.org/web/packages/factoextra/factoextra.pdf) by Alboukadel Kassambara and Fabian Mundt
| **[R Package]** [FactoMineR](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Francois Husson, Julie Josse, Sebastien Le and Jeremy Mazet
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Factor Analysis with the psych package](https://m-clark.github.io/posts/2020-04-10-psych-explained/) by Michael Clark
| **[Article]** [Factor Analysis in R with Psych Package: Measuring Consumer Involvement](https://www.r-bloggers.com/2019/01/factor-analysis-in-r-with-psych-package-measuring-consumer-involvement/) by Peter Prevos
| **[Article]** [Factor Analysis in R](http://jinjian-mu.com/tutorial/2021-04-14-Factor%20Analysis/) by Jinjian Mu
| **[Article]** [How To: Use the psych package for Factor Analysis and Data Reduction](http://personality-project.org/r/psych/HowTo/factor.pdf) by William Revelle
| **[Article]** [A Practical Introduction to Factor Analysis: Exploratory Factor Analysis](https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) by UCLA Advanced Research Computing Team
| **[Article]** [Examining the Big 5 Personality Dataset with Factor Analysis](https://taridwong.github.io/posts/2022-01-01-efacfa/) by Tarid Wongvorachan
| **[Article]** [Principal Component Analysis versus Exploratory Factor Analysis](https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/203-30.pdf) by Diana Suhr
| **[Article]** [Exploratory Factor Analysis](https://www.publichealth.columbia.edu/research/population-health-methods/exploratory-factor-analysis) by Columbia University Irving Medical Center
| **[Article]** [Factor Analysis Example](https://real-statistics.com/multivariate-statistics/factor-analysis/factor-analysis-example/) by Charles Zaiontz
| **[Article]** [Factor Analysis Guide with an Example](https://statisticsbyjim.com/basics/factor-analysis/) by Jim Frost
| **[Article]** [What Is Factor Analysis and How Does It Simplify Research Findings?](https://www.qualtrics.com/experience-management/research/factor-analysis/) by Qualtrics Team
| **[Article]** [How Can I Perform A Factor Analysis With Categorical (Or Categorical And Continuous) Variables?](https://stats.oarc.ucla.edu/stata/faq/how-can-i-perform-a-factor-analysis-with-categorical-or-categorical-and-continuous-variables/) by UCLA Advanced Research Computing Team
| **[Article]** [Factor Analysis on Ordinal Data Example in R (psych, homals)](https://alice86.github.io/2018/04/08/Factor-Analysis-on-Ordinal-Data-example-in-R-(psych,-homals)/) by Jiayu Wu
| **[Publication]** [Dandelion Plot: A Method for the Visualization of R-mode Exploratory Factor Analyses](https://link.springer.com/article/10.1007/s00180-014-0518-x) by Artur Manukyan, Erhan Cene, Ahmet Sedef and Ibrahim Demir (Computational Statistics) 
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|